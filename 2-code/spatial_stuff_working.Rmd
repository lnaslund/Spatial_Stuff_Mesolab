---
title: "Spatial Stuff Working Lab Meeting"
author: "L. Naslund"
date: "3/14/2022"
output: html_document
---

```{r setup, include=FALSE}
# uncomment and install packages you do not already have installed 

# install.packages("sf")
# install.packages("tidyverse")
# install.packages("mapview")
# install.packages("leaflet")
# install.packages("nhdplusTools")
# install.packages("elevatr")
# install.packages("stars")
# install.packages("raster")
# whitebox::install_whitebox()

library(tidyverse)  # data visualization and manipulation tools
library(sf) # the main package to deal with vector data
library(mapview) # quick viewing of spatial data
library(leaflet) # develop pretty, interactive maps 
library(whitebox) # hydrology toolbox
library(nhdplusTools) # download NHDPlus data from R
library(elevatr) # package to access elevation data
library(stars) # package for raster and vector data cubes in S3
library(raster) # package for rasters in S4
library(riverdist) # package for calculating flow path distances
```

```{r, approaches to vector data}
# There are two primary approaches to handling vector data in R. One represented by the package sp and one represented by the package sf. These two packages use different object oriented systems in R which means they (and the packages that are dependent on each) don't play well together. (If you want to learn more about OO systems in R, read this chapter in Hadley Wickham's Advanced R book http://adv-r.had.co.nz/OO-essentials.html)
# sp predates sf and many of the spatial analysis packages you may find were built with sp objects. 
# Several of the packages that are used with sp classes--rgdal, rgeos, and maptools--will be retired by the end of 2023 so you may find that the other packages you use for spatial analysis are/will be changing.
# The code below is to generally illustrate the difference between classes in sp and sf. You do not need to uncomment this and run it. This is getting into the weeds a bit but I find it helpful to know that these two ways of handling spatial data exist.

# library(sp)
# library(pryr)
# 
# random_mat <- matrix(runif(2), ncol=2)
# 
# pts_sp <- sp::SpatialPoints(random_mat, proj4string = CRS("+init=epsg:5070"))
# 
# str(pts_sp) # stores information as lists
# pryr::otype(pts_sp) # S4
# 
# library(sf)
# 
# pts_sf <- sf::st_as_sf(as.data.frame(random_mat), coords=c(1,2), crs=5070)
# 
# str(pts_sf)# dataframe w/ spatial info stored in column
# pryr::otype(pts_sf) # S3
```

```{r, import GPS coordinates convert to vector data}
# import and plot GPS points from csv and call object "sites"




# convert sites into sf object
# EPSG codes are a fast way of retrieving coordinate reference system 
# CRS 4326 is WGS 84




# determine class of sites
# notice addition of sf data type


# plot sites


# create sf object from dataframe created within R









# transform sites to USA Albers Equal Area Conic (EPSG:5070)

```

```{r, import vector data}
# import coweeta watershed from shapefile to object called "coweeta_ws" and transform to Albers Equal Area Conic


# plot watershed and sites


```

```{r, spatial data manipulation}
# sf is the workhorse of spatial data manipulation in R
# This cheat sheet provides a good overview of what is possible in sf https://github.com/rstudio/cheatsheets/blob/main/sf.pdf 

# geometric confirmation
# are all sites within the delineated watershed?


# which sites are within 1km from the confluence


# geometry operations
# create buffer around points


# plot buffers and sites



# create a polygon called "buffs_inter" that is the intersection of polygons for the first and second site


# plot buffers and the intersection of the polygons


```


```{r, crop raster and summarize}
# There are three main raster packages you may come across and my impression is that there isn't a settled favorite yet. Here I am focusing on the stars packages because it implements rasters as S3 objects and so has better integration with sf but there is much about stars I find confusing. You are very likely to also come across terra and raster which both have S4 implementation of rasters. Terra has functions for converting sf objects to S4 so you can use them together but I am avoiding this issue for simplicity. There are also probably other important distinctions between terra/raster and stars that I do not know about.
# Here is a good intro to stars: https://keen-swartz-3146c4.netlify.app/sf.html#package-stars

file_name <- "../1-data/Coweeta_LC/Coweeta_nlcd.tiff"

# load raster using raster
# library(raster)
# lc_raster <- raster(file_name)

# load raster using terra 
# library(terra) # package to deal with rasters, similar functionality to raster but faster 
# lc <- rast(file_name)
# class(lc)
# # pryr::otype(lc) # S4 type

lc_stars <-read_stars(file_name) 
class(lc_stars)
# pryr::otype(lc_stars) # S3 type

plot(lc_stars)

# To crop, the crs need to be identical, in this case they are. But if they weren't you would need the code below to transform the raster and then resample.
st_crs(lc_stars) == st_crs(coweeta_ws)

# Transforming the crs of gridded spatial results in a curvilinear grid. We can use st_warp to resample to get a regular grid. Warping results in some loss of data
# new_grid <- lc_stars %>% st_transform(st_crs(coweeta_ws)) %>% st_bbox() %>% st_as_stars() 
# lc_stars_trans <- lc_stars %>% st_warp(new_grid) 
# 
# plot(lc_stars_trans) # notice the rotation of the raster

# crop the land cover raster using Coweeta watershed polygon
ws_lc <- lc_stars %>% st_crop(coweeta_ws)
# ws_lc <- lc_stars[coweeta_ws]) # base R syntax

plot(droplevels(ws_lc))

# summarize land cover types
legend <- read.csv("../1-data/Coweeta_LC/NLCD_legend.csv")
table(ws_lc) %>% 
  as.tibble() %>% 
  mutate(Value=as.integer(ws_lc)) %>% 
  left_join(legend, by="Value") %>% 
  rename("Class"="Legend") %>% 
  filter(n>0) %>% 
  dplyr::select(Class, n)
```

```{r, watershed delineation}
# original code credit: https://matthewrvross.com/active.html includes code for how to make cool interactive 3D watersheds
# Run time on my machine is about 14 sec. Take that Arcgis :)

# NOTE: NHDPlus HR comes with flow accumulation and flow direction rasters. I would highly recommend starting with these rasters if you are working with US data but I am showing how to generate these rasters in case you aren't working with US data. 

# Z sets resolution with 14 representing highest resolution
# expand argument extends the bounding box used to extract elevation data


# check that all the sites were captured
# especially if you are extracting elevation from a large area, you will want to generate a bounding box (st_bbox(coweeta_dem)) and plot that because it will take forever for the raster to plot.



# view raster directly with sites plotted on top
# elevatr is downloading the DEM as an S4 object. The raster package comes with built in plotting method for S4 objects



# create folder called "delin" to keep delineation files within the data folder


# write dem raster to a file in the delin folder to access later


# write pour points to a shapefile in the delin folder


# double check that the CRS of the dem and sites are the same using compareCRS in the raster package



# create filepath to dem


# fill single cell pits


# flow direction raster


# flow accumulation raster


# snap pour points
# if you are working with an existing flow accumulation raster this is where you would start
# the last parameter is the max snapping distance (in map units). This parameter is super important and may need to be adjusted to get a correct delineation. 


# create folder called "basins" inside delin folder to store basins


# extract watersheds from pour points


# get list of watersheds


# function to transform raster outlines into shapefiles
shed_stacker <- function(x){
  stars::read_stars(sheds[x]) %>%
    st_as_sf(merge=T,use_integer = T) %>%
    rename(id=1) %>%
    group_by(id) %>%
    summarize()
}

# apply transformation
s <- purrr::map(1:length(sheds),shed_stacker)

# bind watersheds together into sf object
shape_sheds <- do.call('rbind',s) %>% arrange(id)
shape_sheds$site <- sites$Site

# plot watersheds
mapview(shape_sheds, zcol="site")
```

```{r, make pretty maps}
# There are SO MANY packages for making maps in R. If you are interested in quick static maps that won't take a ton of time to learn how to do, you might look into tmaps which uses ggplot syntax
# Today I'm focusing on leaflet because it has (to my knowledge) the greatest flexibility in making interactive maps and takes a little more activation energy to learn. It is also well integrated with shiny, in case that is something you care about.

# load temperature data
temp_data <- read.csv("../1-data/temp_summary.csv") %>% dplyr::select(-X) %>% rename("Site" = "ws")
  
# join temperature data to sites object

# calculate watershed area and temperature data






# create color palette function for continuous variable





# set popups for map




# plot map
# leaflet expects everything in WGS 84 (EPSG: 4326). If you need to display data in a different projection, you need to specify the projection in leafletCRS function and include it as an argument to "options" in the leaflet() function
# https://rstudio.github.io/leaflet/projections.html 





















  
```

```{r, nhdplus tools to download flowlines}
# You can use nhdplus tools to download HR data too but last time I checked, you have to download the entire HUC04 network and then subset, which takes forever, which is why I am focusing on NHDPlus V2 data. More info https://usgs-r.github.io/nhdplusTools/index.html 

# Extract the ComID of the outlet flowline segment
# ComID is an identifier of an NHD flowlines


# Extract upstream flowline segments





# Download flowline, catchment, and waterbody layers 







# No waterbodies in this upstream trace
# notice the attributes that are downloaded (e.g., stream order, QAMA) 


#plot flowline

```

```{r, river dist}
# This code just scratches the surface of what you can do with riverdist and also uses a very simple example. See vignette and documentation for more information https://cran.r-project.org/web/packages/riverdist/vignettes/riverdist_vignette.html 

# write NHD flowline as shp file for river dist


# read in flowline shp


# plot flowline and sites. Notice that the NHDPlus V2 did not capture some of the sampled tributaries



# convert shapefile to rivernetwork


# clean up topology



# save network
save(abstreams_fixed, file = "../1-data/fixed.rda")

# load network



# find projected site coordinates and convert to data frame called "site_coord"



# snap sites to vertices


# plot 




# create distance matrix


# plot flowpath between sites


```

